{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd # used to load the data\n",
    "import numpy as np # optimized numerical library\n",
    "\n",
    "from sklearn import preprocessing, metrics, utils, decomposition, model_selection, linear_model, discriminant_analysis, svm, tree, ensemble # library providing several ML algorithms and related utility\n",
    "\n",
    "from imblearn import over_sampling # provides several resampling techniques to cope with unbalanced datasets (https://github.com/scikit-learn-contrib/imbalanced-learn) compatible with sklearn\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt # used for plotting\n",
    "\n",
    "# Start by defining three helper functions:\n",
    "# - one to plot the sample distribution  acorss the class labels (to see how un-/balanced the dataset is)\n",
    "# - one to compute and plot the confusion matrix\n",
    "# - one to plot data in 2D with different colors per class label\n",
    "\n",
    "def plot_pie(y, labels, title=\"\"):\n",
    "    target_stats = Counter(y)\n",
    "    sizes = list(target_stats.values())\n",
    "    explode = tuple([0.1] * len(target_stats))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title + \" (size: %d)\" % len(y))\n",
    "    ax.pie(sizes, explode=explode, labels=target_stats.keys(), shadow=True, autopct='%1.1f%%')\n",
    "    ax.axis('equal')\n",
    "\n",
    "\n",
    "def compute_and_plot_cm(ytest, ypred, labels, title=\"\"):\n",
    "    global nfigure\n",
    "    # Compute confusion matrix\n",
    "    cm = metrics.confusion_matrix(ytest, ypred)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(ytest, ypred, normalize=True)\n",
    "\n",
    "    # Normalize the matrix\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "\n",
    "    nfigure = nfigure + 1\n",
    "    plt.figure(nfigure) # new numbered figure\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) # plot the confusionmatrix using blue shaded colors\n",
    "    plt.title(\"Confusion Matrix Normalized (%s) Accuracy: %.1f%%\" % (title, accuracy*100)) # add title\n",
    "    plt.colorbar() # plot the color bar as legend\n",
    "\n",
    "    # Plot the x and y ticks using the class label names\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "\n",
    "def plot_2d(xpred, ypred, labels, title=\"\"):\n",
    "    global nfigure\n",
    "    # define the colors to use for each class label\n",
    "    colors = ['red', 'blue', 'green', 'yellow', 'black']\n",
    "    len_colors = len(colors)\n",
    "    if len_colors < len(labels):\n",
    "        print(\"WARNING: we have less colors than classes: some classes will reuse the same color\")\n",
    "\n",
    "    nfigure = nfigure + 1\n",
    "    plt.figure(nfigure) # new numbered figure\n",
    "    plt.title(\"Feature Space (%s)\" % title) # add title\n",
    "\n",
    "\n",
    "    # plot each class label with a separate color \n",
    "    for c in range(len(labels)):\n",
    "        cur_class = (ypred == c) # get all points belonging to class c\n",
    "        plt.plot(xpred[cur_class, 0], xpred[cur_class, 1], 'o', color=colors[c % len_colors]) # plot class c\n",
    "\n",
    "\n",
    "nfigure = 0 #used to number the figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Load data ####################\n",
    "# Get the dataset loaded and define class labels \n",
    "data = pd.read_csv('data/jobs.csv', header=0)\n",
    "data_class_labels = [\"evict\", \"fail\", \"finish\", \"kill\"]\n",
    "\n",
    "# All data columns except last are input features (X), last column is output label (y)\n",
    "n_features = len(data.columns) - 1\n",
    "\n",
    "X = data.iloc[:,0:n_features]\n",
    "y = data.iloc[:,n_features]\n",
    "\n",
    "y = y - 2 #First two labels not in dataset, shift by two\n",
    "\n",
    "plot_pie(y, data_class_labels, \"Original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What problem do you see? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets make the data balance: over_sampling.SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Resample data #################\n",
    "\n",
    "# Google data is very skewed, try to balance the dataset\n",
    "sm = over_sampling.SMOTE(random_state=42, ratio=\"auto\")\n",
    "X, y = sm.fit_sample(X, y)\n",
    "\n",
    "# Plot the balanced label distribution\n",
    "plot_pie(y, data_class_labels, \"Balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets have a smaller number of samples:  utils.resample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data with simple random resampling (if too big)\n",
    "# - replace decideds if sampling with or without replacement\n",
    "# - n_samples decide the size of the ouput: if set to None ouput = input (i.e. no resampling)\n",
    "X, y = utils.resample(X, y, replace=False, n_samples=10000)\n",
    "\n",
    "# Plot the resampled label distribution\n",
    "\n",
    "plot_pie(y, data_class_labels, \"Sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Split data ####################\n",
    "# Split data in training and testing\n",
    "#X_train, X_test, y_train, y_test = model_selection.train_test_split( , , , )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Scale data ####################\n",
    "# Train a scaler to standardize the features (zero mean and unit variance)\n",
    "#scaler = preprocessing.StandardScaler().fit()\n",
    "\n",
    "# ... and scale the features\n",
    "#X_train_scaled = scaler.transform()\n",
    "#X_test_scaled = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the 2 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ PCA ####################\n",
    "# Train a PCA with 2 dimensions\n",
    "#pca = decomposition.PCA(n_components=).fit()\n",
    "\n",
    "# ... and apply it to the features\n",
    "#X_train_scaled_pca = pca.transform()\n",
    "#X_test_scaled_pca = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Logit ##################\n",
    "# Train a Logit model on the original features\n",
    "#lr = linear_model.LogisticRegression().fit(,)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_lr = lr.predict()\n",
    "\n",
    "# Prit the accuracy\n",
    "#print(\"Acuracy of LR : %.1f%%\" % (metrics.accuracy_score(,)*100))\n",
    "\n",
    "#Compute and print and confusion matrix\n",
    "#compute_and_plot_cm(, , , title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LR on PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logit model on pca extracted features\n",
    "#lr_pca = linear_model.LogisticRegression().fit(X_train_scaled_pca, y_train)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_lr_pca = lr_pca.predict(X_test_scaled_pca)\n",
    "\n",
    "# Prit the accuracy\n",
    "#print(\"Acuracy of LR + PCA: %.1f%%\" % (metrics.accuracy_score(y_test,y_lr_pca)*100))\n",
    "\n",
    "\n",
    "#Compute and print and confusion matrix\n",
    "#compute_and_plot_cm(y_test, y_lr_pca, data_class_labels, title=\"LR + PCA\")\n",
    "\n",
    "# visualize the predictions based on 2 PCA components\n",
    "#plot_2d(X_test_scaled_pca, y_lr_pca, data_class_labels, title=\"LR + PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Apply LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ LDA ##################\n",
    "# Train an LDA model on original features\n",
    "#lda = discriminant_analysis.LinearDiscriminantAnalysis().fit(,)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_lda = lda\n",
    "\n",
    "# Prit the accuracy\n",
    "#print(\"Acuracy of LDA : %.1f%%\" % (metrics.accuracy_score(,)*100))\n",
    "\n",
    "#Compute and print and confusion matrix\n",
    "#compute_and_plot_cm(, , , title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LDA on 2 princinple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model on pca extracted features\n",
    "#lda_pca = discriminant_analysis.LinearDiscriminantAnalysis().fit(X_train_scaled_pca, y_train)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_lda_pca = lda_pca.predict(X_test_scaled_pca)\n",
    "\n",
    "# Pring the accuracy\n",
    "#print(\"Acuracy of LDA + PCA: %.1f%%\" % (metrics.accuracy_score(y_test,y_lda_pca)*100))\n",
    "\n",
    "#Compute and print and confusion matrix\n",
    "#compute_and_plot_cm(y_test, y_lda_pca, data_class_labels, title=\"LDA + PCA\")\n",
    "\n",
    "# visualize the predictions based on 2 PCA components\n",
    "#plot_2d(X_test_scaled_pca, y_lda_pca, data_class_labels, title=\"LDA + PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apply QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ QDA ##################\n",
    "# Train a QDA model on original features\n",
    "#qda = discriminant_analysis.QuadraticDiscriminantAnalysis().fit(,)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_qda =\n",
    "\n",
    "# Print the accuracy\n",
    "\n",
    "#Compute and print and confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LDA on 2 princinple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a QDA model on pca extracted features\n",
    "#qda_pca = discriminant_analysis.QuadraticDiscriminantAnalysis().fit(X_train_scaled_pca, y_train)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_qda_pca = qda_pca.predict(X_test_scaled_pca)\n",
    "\n",
    "# Print the accuracy\n",
    "#print(\"Acuracy of QDA + PCA: %.1f%%\" % (metrics.accuracy_score(y_test,y_qda_pca)*100))\n",
    "\n",
    "#Compute and print and confusion matrix\n",
    "#compute_and_plot_cm(y_test, y_qda_pca, data_class_labels, title=\"QDA + PCA\")\n",
    "\n",
    "#plot_2d(X_test_scaled_pca, y_qda_pca, data_class_labels, title=\"QDA + PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Applyd ELDA\n",
    "### Lets get the expanded bases from the 2 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Polynomial expanded features ##################\n",
    "# Train a polynomial expansion on original features\n",
    "#poly2 = preprocessing.PolynomialFeatures(degree=).fit()\n",
    "\n",
    "# ... and apply it to the features\n",
    "#X_train_scaled_poly2 = poly2.transform()\n",
    "#X_test_scaled_poly2 = poly2.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################  LDA on expanded ##################\n",
    "# Train an LDA model on the original expanded features\n",
    "#lda_poly2 = \n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_lda_poly2 = lda_poly2\n",
    "\n",
    "# Print the accuracy\n",
    "\n",
    "#Compute and print and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Apply Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ SVM ##################\n",
    "# Train a SVM model on the original features\n",
    "#sv = svm.SVC().fit(,)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_sv = sv\n",
    "\n",
    "# Print the accuracy\n",
    "\n",
    "#Compute and print and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Apply Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ DecisionTree ##################\n",
    "# Train a DT model on the original features\n",
    "#dt = tree.DecisionTreeClassifier(max_depth=).fit(, )\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_dt = dt\n",
    "\n",
    "#print the accuracy\n",
    "\n",
    "# Compute and show confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Apply Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ RandomForest ##################\n",
    "# Train a RF model on the original features\n",
    "#rf = ensemble.RandomForestClassifier().fit(,)\n",
    "\n",
    "# Compute the predicted labels on test data\n",
    "#y_rf = rf\n",
    "\n",
    "#print the accuracy\n",
    "\n",
    "# Compute and show confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
